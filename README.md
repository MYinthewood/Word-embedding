# Word-embedding
This notebook explain what is word embedding and why.
What is Word embedding?
To use text data as imput to a Neural network model, we need to convert text to numbers first. There is a very old way to convert text to numbers, that is one-hot encoding. However, unlike ML models, passing a sparse vectors of huge sizes to Neural network can greatly affect the final result. Therefore, we need to convert our text to small dense vectors. Therefore we need word Embeddings. Every token is represented as a n-diimensional dense vector, and each one has n-dimensional parameters explaining its characteristics.
![image](https://github.com/MYinthewood/Word-embedding/assets/129473659/e464587c-b7b7-489a-a10a-b7654ccf25b3)
![image](https://github.com/MYinthewood/Word-embedding/assets/129473659/c5ebfd0b-08c4-4c2c-89bd-7e5ba520254f)
![image](https://github.com/MYinthewood/Word-embedding/assets/129473659/1ce070e4-f724-4b17-9ea6-19bbb06fbbd8)
